{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLG454E Learning From Data Homework 1\n",
    "\n",
    "Before starting, read the README.txt file. If you have any questions, send an e-mail at kamard@itu.edu.tr.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Simple Linear Regression  (30 points)\n",
    "\n",
    "### Background\n",
    "\n",
    "Remember from the course material: In Linear Regression, we aim to find the line that fits the given data as well as possible. The line formula is:\n",
    "\n",
    "### <center> $ \\hat{y} = f(x) = \\beta_0 + \\beta_1 x $ <center> \n",
    "\n",
    "And using linear regression, the goal is to find the $\\beta_0$ and $\\beta_1$ such that the the Mean Squared Error (MSE) is minimized:\n",
    "    \n",
    "### <center> $ MSE \\Rightarrow \\mathcal{L} = \\dfrac{1}{N} \\sum_{i=1}^{N} (y_i - f(x_i ))^2 $ <center> \n",
    "    \n",
    "As the MSE error gets smaller, the line starts to represent the data better and better.\n",
    "    \n",
    "Consider $\\beta = (\\beta_0 + \\beta_1)$. In order to find the $\\beta$ that minimizes the MSE loss, we use **Gradient Descent**. Iteratively $\\beta$ is updated in the opposite direction of the gradient, while the size of the update is controlled with the learning rate $\\eta$:\n",
    "    \n",
    "### <center> $ \\beta \\leftarrow \\beta-\\eta \\frac{d \\mathcal{L}}{d \\beta} $ <center> \n",
    "\n",
    "### Your Task\n",
    "\n",
    "In this part, you are going to code up Simple Linear Regression on the data given in **data1.csv**, and find the line that fits the data best. To do so, you'll need to complete the following:\n",
    "\n",
    "- Code up the gradient descent, and print the loss value at every time step. You can use the number of steps as the stopping criteria (e.g. stopping gradient descent after 10 steps). \n",
    "- At the end of the algorithm, plot the found line with the data.\n",
    "    \n",
    "Given learning rate and number of iterations as default values in the code should work, but you are free to change them to make your algorithm faster if you'd like. You are not allowed to use any libraries other than the imported ones at the beginning. You can initialize the $\\beta$ randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    df = pd.read_csv(\"data1.csv\")\n",
    "    x = df['x'].values\n",
    "    y = df['y'].values\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def gradient_descent(x, y, learning_rate = 0.1, max_its = 500):\n",
    "    # Initialize the beta values\n",
    "    beta0 = 5\n",
    "    beta1 = 5\n",
    "    \n",
    "    # start gradient descent loop\n",
    "    for k in range(0,max_its):\n",
    "        # Step 1: calculate derivative\n",
    "        y_hat = beta0 + (beta1 * x)\n",
    "        y_diff = y_hat - y\n",
    "        \n",
    "        db0 = np.sum(y_diff) * (2 / len(x))\n",
    "        db1 = np.dot(y_diff, x) * (2 / len(x))\n",
    "        \n",
    "        # Step 2: Update weights\n",
    "        beta0 = beta0 - learning_rate * db0\n",
    "        beta1 = beta1 - learning_rate * db1\n",
    "        \n",
    "        # Step 3: Calculate and print the loss value\n",
    "        loss = np.sum((y - y_hat) ** 2) / len(x)\n",
    "        print(f\"Training cost of iteration {k + 1}: {loss}\")\n",
    "\n",
    "    return beta0, beta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training cost of iteration 1: 30.586111484834905\n",
      "Training cost of iteration 2: 17.90263526126138\n",
      "Training cost of iteration 3: 10.531293654539336\n",
      "Training cost of iteration 4: 6.245900418996951\n",
      "Training cost of iteration 5: 3.753243860022202\n",
      "Training cost of iteration 6: 2.3020847579831174\n",
      "Training cost of iteration 7: 1.4560201166760804\n",
      "Training cost of iteration 8: 0.9615378869594027\n",
      "Training cost of iteration 9: 0.671368893358638\n",
      "Training cost of iteration 10: 0.4999613505694163\n",
      "Training cost of iteration 11: 0.39761565076998673\n",
      "Training cost of iteration 12: 0.3354590523310865\n",
      "Training cost of iteration 13: 0.2967173544453594\n",
      "Training cost of iteration 14: 0.2716445415438973\n",
      "Training cost of iteration 15: 0.25457737180689244\n",
      "Training cost of iteration 16: 0.24222436281516693\n",
      "Training cost of iteration 17: 0.23267184845561306\n",
      "Training cost of iteration 18: 0.2248065883121462\n",
      "Training cost of iteration 19: 0.21798035577236244\n",
      "Training cost of iteration 20: 0.21181509354253453\n",
      "Training cost of iteration 21: 0.2060897259263566\n",
      "Training cost of iteration 22: 0.20067440619961957\n",
      "Training cost of iteration 23: 0.19549231943815712\n",
      "Training cost of iteration 24: 0.1904974925541602\n",
      "Training cost of iteration 25: 0.1856619030719209\n",
      "Training cost of iteration 26: 0.18096798963886568\n",
      "Training cost of iteration 27: 0.17640430047025213\n",
      "Training cost of iteration 28: 0.17196296466640046\n",
      "Training cost of iteration 29: 0.16763822247232252\n",
      "Training cost of iteration 30: 0.16342557070605512\n",
      "Training cost of iteration 31: 0.15932126556346576\n",
      "Training cost of iteration 32: 0.15532203304544298\n",
      "Training cost of iteration 33: 0.15142490001362074\n",
      "Training cost of iteration 34: 0.14762709533872548\n",
      "Training cost of iteration 35: 0.14392599178435794\n",
      "Training cost of iteration 36: 0.1403190715719232\n",
      "Training cost of iteration 37: 0.13680390571928927\n",
      "Training cost of iteration 38: 0.13337814139743956\n",
      "Training cost of iteration 39: 0.13003949396113787\n",
      "Training cost of iteration 40: 0.12678574171065102\n",
      "Training cost of iteration 41: 0.12361472225544229\n",
      "Training cost of iteration 42: 0.12052432982354987\n",
      "Training cost of iteration 43: 0.1175125131350229\n",
      "Training cost of iteration 44: 0.11457727361735266\n",
      "Training cost of iteration 45: 0.11171666383353465\n",
      "Training cost of iteration 46: 0.10892878604726026\n",
      "Training cost of iteration 47: 0.10621179088103039\n",
      "Training cost of iteration 48: 0.1035638760411744\n",
      "Training cost of iteration 49: 0.10098328509433296\n",
      "Training cost of iteration 50: 0.09846830628611264\n",
      "Training cost of iteration 51: 0.0960172713962045\n",
      "Training cost of iteration 52: 0.09362855462634505\n",
      "Training cost of iteration 53: 0.09130057151871959\n",
      "Training cost of iteration 54: 0.08903177790312602\n",
      "Training cost of iteration 55: 0.08682066887163879\n",
      "Training cost of iteration 56: 0.08466577777976717\n",
      "Training cost of iteration 57: 0.08256567527325702\n",
      "Training cost of iteration 58: 0.08051896833977894\n",
      "Training cost of iteration 59: 0.0785242993848115\n",
      "Training cost of iteration 60: 0.07658034533106844\n",
      "Training cost of iteration 61: 0.07468581674085126\n",
      "Training cost of iteration 62: 0.0728394569607326\n",
      "Training cost of iteration 63: 0.07104004128799582\n",
      "Training cost of iteration 64: 0.06928637615827327\n",
      "Training cost of iteration 65: 0.06757729835384253\n",
      "Training cost of iteration 66: 0.06591167423205367\n",
      "Training cost of iteration 67: 0.0642883989733748\n",
      "Training cost of iteration 68: 0.06270639584855724\n",
      "Training cost of iteration 69: 0.061164615504433105\n",
      "Training cost of iteration 70: 0.05966203526787229\n",
      "Training cost of iteration 71: 0.058197658467436256\n",
      "Training cost of iteration 72: 0.05677051377227928\n",
      "Training cost of iteration 73: 0.055379654547858065\n",
      "Training cost of iteration 74: 0.05402415822802278\n",
      "Training cost of iteration 75: 0.05270312570307292\n",
      "Training cost of iteration 76: 0.05141568072337166\n",
      "Training cost of iteration 77: 0.05016096931812379\n",
      "Training cost of iteration 78: 0.04893815922893114\n",
      "Training cost of iteration 79: 0.047746439357750164\n",
      "Training cost of iteration 80: 0.046585019228885145\n",
      "Training cost of iteration 81: 0.04545312846466031\n",
      "Training cost of iteration 82: 0.0443500162744235\n",
      "Training cost of iteration 83: 0.04327495095654154\n",
      "Training cost of iteration 84: 0.04222721941305767\n",
      "Training cost of iteration 85: 0.04120612667668905\n",
      "Training cost of iteration 86: 0.040210995449850316\n",
      "Training cost of iteration 87: 0.039241165655397585\n",
      "Training cost of iteration 88: 0.038295993998794915\n",
      "Training cost of iteration 89: 0.0373748535414127\n",
      "Training cost of iteration 90: 0.036477133284675285\n",
      "Training cost of iteration 91: 0.0356022377647814\n",
      "Training cost of iteration 92: 0.03474958665772915\n",
      "Training cost of iteration 93: 0.033918614394383255\n",
      "Training cost of iteration 94: 0.03310876978532916\n",
      "Training cost of iteration 95: 0.032319515655265744\n",
      "Training cost of iteration 96: 0.031550328486693015\n",
      "Training cost of iteration 97: 0.030800698072659716\n",
      "Training cost of iteration 98: 0.030070127178339616\n",
      "Training cost of iteration 99: 0.029358131211212325\n",
      "Training cost of iteration 100: 0.028664237899629873\n",
      "Training cost of iteration 101: 0.027987986979555783\n",
      "Training cost of iteration 102: 0.027328929889268928\n",
      "Training cost of iteration 103: 0.026686629471829506\n",
      "Training cost of iteration 104: 0.02606065968510999\n",
      "Training cost of iteration 105: 0.025450605319198495\n",
      "Training cost of iteration 106: 0.02485606172098738\n",
      "Training cost of iteration 107: 0.0242766345257641\n",
      "Training cost of iteration 108: 0.02371193939562647\n",
      "Training cost of iteration 109: 0.023161601764548782\n",
      "Training cost of iteration 110: 0.022625256589929733\n",
      "Training cost of iteration 111: 0.022102548110457226\n",
      "Training cost of iteration 112: 0.021593129610129463\n",
      "Training cost of iteration 113: 0.021096663188275952\n",
      "Training cost of iteration 114: 0.02061281953542583\n",
      "Training cost of iteration 115: 0.02014127771487462\n",
      "Training cost of iteration 116: 0.019681724949804826\n",
      "Training cost of iteration 117: 0.019233856415818932\n",
      "Training cost of iteration 118: 0.018797375038747483\n",
      "Training cost of iteration 119: 0.018371991297597656\n",
      "Training cost of iteration 120: 0.01795742303251223\n",
      "Training cost of iteration 121: 0.01755339525761096\n",
      "Training cost of iteration 122: 0.017159639978590668\n",
      "Training cost of iteration 123: 0.016775896014962818\n",
      "Training cost of iteration 124: 0.016401908826810814\n",
      "Training cost of iteration 125: 0.016037430345951936\n",
      "Training cost of iteration 126: 0.015682218811392076\n",
      "Training cost of iteration 127: 0.015336038608964129\n",
      "Training cost of iteration 128: 0.014998660115043399\n",
      "Training cost of iteration 129: 0.014669859544236935\n",
      "Training cost of iteration 130: 0.014349418800945075\n",
      "Training cost of iteration 131: 0.014037125334697125\n",
      "Training cost of iteration 132: 0.013732771999165345\n",
      "Training cost of iteration 133: 0.013436156914763403\n",
      "Training cost of iteration 134: 0.01314708333473823\n",
      "Training cost of iteration 135: 0.012865359514666639\n",
      "Training cost of iteration 136: 0.012590798585269998\n",
      "Training cost of iteration 137: 0.012323218428462528\n",
      "Training cost of iteration 138: 0.012062441556551283\n",
      "Training cost of iteration 139: 0.011808294994507262\n",
      "Training cost of iteration 140: 0.011560610165230006\n",
      "Training cost of iteration 141: 0.01131922277772924\n",
      "Training cost of iteration 142: 0.011083972718149626\n",
      "Training cost of iteration 143: 0.010854703943566118\n",
      "Training cost of iteration 144: 0.010631264378479716\n",
      "Training cost of iteration 145: 0.010413505813944714\n",
      "Training cost of iteration 146: 0.010201283809260766\n",
      "Training cost of iteration 147: 0.009994457596164422\n",
      "Training cost of iteration 148: 0.009792889985456552\n",
      "Training cost of iteration 149: 0.009596447276003908\n",
      "Training cost of iteration 150: 0.009404999166054213\n",
      "Training cost of iteration 151: 0.009218418666806179\n",
      "Training cost of iteration 152: 0.009036582018176984\n",
      "Training cost of iteration 153: 0.008859368606711408\n",
      "Training cost of iteration 154: 0.008686660885578075\n",
      "Training cost of iteration 155: 0.008518344296599922\n",
      "Training cost of iteration 156: 0.008354307194266964\n",
      "Training cost of iteration 157: 0.008194440771681095\n",
      "Training cost of iteration 158: 0.008038638988383744\n",
      "Training cost of iteration 159: 0.007886798500018565\n",
      "Training cost of iteration 160: 0.007738818589782394\n",
      "Training cost of iteration 161: 0.0075946011016191535\n",
      "Training cost of iteration 162: 0.00745405037511229\n",
      "Training cost of iteration 163: 0.007317073182032591\n",
      "Training cost of iteration 164: 0.00718357866449925\n",
      "Training cost of iteration 165: 0.007053478274713259\n",
      "Training cost of iteration 166: 0.006926685716223061\n",
      "Training cost of iteration 167: 0.006803116886683558\n",
      "Training cost of iteration 168: 0.006682689822070412\n",
      "Training cost of iteration 169: 0.006565324642312825\n",
      "Training cost of iteration 170: 0.006450943498308533\n",
      "Training cost of iteration 171: 0.006339470520285977\n",
      "Training cost of iteration 172: 0.006230831767479388\n",
      "Training cost of iteration 173: 0.006124955179083387\n",
      "Training cost of iteration 174: 0.006021770526454563\n",
      "Training cost of iteration 175: 0.005921209366528359\n",
      "Training cost of iteration 176: 0.005823204996420322\n",
      "Training cost of iteration 177: 0.005727692409181667\n",
      "Training cost of iteration 178: 0.005634608250679705\n",
      "Training cost of iteration 179: 0.005543890777574626\n",
      "Training cost of iteration 180: 0.005455479816364756\n",
      "Training cost of iteration 181: 0.005369316723473055\n",
      "Training cost of iteration 182: 0.005285344346348447\n",
      "Training cost of iteration 183: 0.005203506985556174\n",
      "Training cost of iteration 184: 0.005123750357831949\n",
      "Training cost of iteration 185: 0.005046021560075538\n",
      "Training cost of iteration 186: 0.0049702690342597975\n",
      "Training cost of iteration 187: 0.004896442533231847\n",
      "Training cost of iteration 188: 0.004824493087383831\n",
      "Training cost of iteration 189: 0.004754372972171047\n",
      "Training cost of iteration 190: 0.00468603567645594\n",
      "Training cost of iteration 191: 0.004619435871656949\n",
      "Training cost of iteration 192: 0.004554529381681766\n",
      "Training cost of iteration 193: 0.004491273153625047\n",
      "Training cost of iteration 194: 0.004429625229211098\n",
      "Training cost of iteration 195: 0.004369544716962741\n",
      "Training cost of iteration 196: 0.0043109917650776515\n",
      "Training cost of iteration 197: 0.004253927534994426\n",
      "Training cost of iteration 198: 0.004198314175630739\n",
      "Training cost of iteration 199: 0.004144114798276406\n",
      "Training cost of iteration 200: 0.004091293452124896\n",
      "Training cost of iteration 201: 0.004039815100426934\n",
      "Training cost of iteration 202: 0.003989645597250353\n",
      "Training cost of iteration 203: 0.003940751664830895\n",
      "Training cost of iteration 204: 0.0038931008714988636\n",
      "Training cost of iteration 205: 0.0038466616101669464\n",
      "Training cost of iteration 206: 0.0038014030773650515\n",
      "Training cost of iteration 207: 0.0037572952528081537\n",
      "Training cost of iteration 208: 0.003714308879483634\n",
      "Training cost of iteration 209: 0.003672415444244925\n",
      "Training cost of iteration 210: 0.0036315871588985344\n",
      "Training cost of iteration 211: 0.0035917969417719586\n",
      "Training cost of iteration 212: 0.0035530183997502353\n",
      "Training cost of iteration 213: 0.0035152258107692003\n",
      "Training cost of iteration 214: 0.0034783941067538958\n",
      "Training cost of iteration 215: 0.0034424988569907223\n",
      "Training cost of iteration 216: 0.003407516251922413\n",
      "Training cost of iteration 217: 0.003373423087354959\n",
      "Training cost of iteration 218: 0.0033401967490661543\n",
      "Training cost of iteration 219: 0.003307815197805407\n",
      "Training cost of iteration 220: 0.0032762569546749666\n",
      "Training cost of iteration 221: 0.0032455010868828265\n",
      "Training cost of iteration 222: 0.0032155271938578587\n",
      "Training cost of iteration 223: 0.0031863153937179917\n",
      "Training cost of iteration 224: 0.0031578463100824152\n",
      "Training cost of iteration 225: 0.0031301010592191198\n",
      "Training cost of iteration 226: 0.003103061237519146\n",
      "Training cost of iteration 227: 0.0030767089092893995\n",
      "Training cost of iteration 228: 0.003051026594855764\n",
      "Training cost of iteration 229: 0.003025997258968704\n",
      "Training cost of iteration 230: 0.003001604299503705\n",
      "Training cost of iteration 231: 0.0029778315364489406\n",
      "Training cost of iteration 232: 0.002954663201172976\n",
      "Training cost of iteration 233: 0.0029320839259653507\n",
      "Training cost of iteration 234: 0.00291007873384303\n",
      "Training cost of iteration 235: 0.0028886330286161197\n",
      "Training cost of iteration 236: 0.0028677325852060685\n",
      "Training cost of iteration 237: 0.002847363540210138\n",
      "Training cost of iteration 238: 0.0028275123827056553\n",
      "Training cost of iteration 239: 0.0028081659452881664\n",
      "Training cost of iteration 240: 0.002789311395337406\n",
      "Training cost of iteration 241: 0.002770936226505318\n",
      "Training cost of iteration 242: 0.002753028250420525\n",
      "Training cost of iteration 243: 0.002735575588603697\n",
      "Training cost of iteration 244: 0.0027185666645884858\n",
      "Training cost of iteration 245: 0.002701990196242759\n",
      "Training cost of iteration 246: 0.0026858351882851055\n",
      "Training cost of iteration 247: 0.002670090924991585\n",
      "Training cost of iteration 248: 0.0026547469630879166\n",
      "Training cost of iteration 249: 0.0026397931248223843\n",
      "Training cost of iteration 250: 0.0026252194912148768\n",
      "Training cost of iteration 251: 0.0026110163954775434\n",
      "Training cost of iteration 252: 0.0025971744166027664\n",
      "Training cost of iteration 253: 0.0025836843731141448\n",
      "Training cost of iteration 254: 0.0025705373169763396\n",
      "Training cost of iteration 255: 0.0025577245276598086\n",
      "Training cost of iteration 256: 0.0025452375063564035\n",
      "Training cost of iteration 257: 0.0025330679703420666\n",
      "Training cost of iteration 258: 0.0025212078474828293\n",
      "Training cost of iteration 259: 0.0025096492708805273\n",
      "Training cost of iteration 260: 0.0024983845736546275\n",
      "Training cost of iteration 261: 0.002487406283856736\n",
      "Training cost of iteration 262: 0.0024767071195144087\n",
      "Training cost of iteration 263: 0.0024662799838009552\n",
      "Training cost of iteration 264: 0.002456117960328105\n",
      "Training cost of iteration 265: 0.0024462143085582687\n",
      "Training cost of iteration 266: 0.002436562459333534\n",
      "Training cost of iteration 267: 0.0024271560105182852\n",
      "Training cost of iteration 268: 0.00241798872275263\n",
      "Training cost of iteration 269: 0.002409054515313777\n",
      "Training cost of iteration 270: 0.0024003474620826726\n",
      "Training cost of iteration 271: 0.002391861787613149\n",
      "Training cost of iteration 272: 0.002383591863301045\n",
      "Training cost of iteration 273: 0.002375532203650698\n",
      "Training cost of iteration 274: 0.002367677462636404\n",
      "Training cost of iteration 275: 0.0023600224301563457\n",
      "Training cost of iteration 276: 0.0023525620285766964\n",
      "Training cost of iteration 277: 0.0023452913093636044\n",
      "Training cost of iteration 278: 0.0023382054498007846\n",
      "Training cost of iteration 279: 0.002331299749790582\n",
      "Training cost of iteration 280: 0.0023245696287363775\n",
      "Training cost of iteration 281: 0.002318010622504225\n",
      "Training cost of iteration 282: 0.0023116183804617885\n",
      "Training cost of iteration 283: 0.0023053886625925175\n",
      "Training cost of iteration 284: 0.002299317336683221\n",
      "Training cost of iteration 285: 0.0022934003755831515\n",
      "Training cost of iteration 286: 0.002287633854532762\n",
      "Training cost of iteration 287: 0.002282013948560388\n",
      "Training cost of iteration 288: 0.002276536929945121\n",
      "Training cost of iteration 289: 0.0022711991657441795\n",
      "Training cost of iteration 290: 0.002265997115383161\n",
      "Training cost of iteration 291: 0.0022609273283075533\n",
      "Training cost of iteration 292: 0.0022559864416939633\n",
      "Training cost of iteration 293: 0.002251171178219521\n",
      "Training cost of iteration 294: 0.0022464783438880236\n",
      "Training cost of iteration 295: 0.0022419048259113305\n",
      "Training cost of iteration 296: 0.0022374475906446244\n",
      "Training cost of iteration 297: 0.002233103681574194\n",
      "Training cost of iteration 298: 0.0022288702173563313\n",
      "Training cost of iteration 299: 0.0022247443899061415\n",
      "Training cost of iteration 300: 0.0022207234625349105\n",
      "Training cost of iteration 301: 0.002216804768134835\n",
      "Training cost of iteration 302: 0.0022129857074099303\n",
      "Training cost of iteration 303: 0.0022092637471518824\n",
      "Training cost of iteration 304: 0.002205636418559759\n",
      "Training cost of iteration 305: 0.0022021013156024497\n",
      "Training cost of iteration 306: 0.0021986560934227077\n",
      "Training cost of iteration 307: 0.002195298466781809\n",
      "Training cost of iteration 308: 0.002192026208543733\n",
      "Training cost of iteration 309: 0.002188837148197883\n",
      "Training cost of iteration 310: 0.0021857291704193713\n",
      "Training cost of iteration 311: 0.0021827002136659292\n",
      "Training cost of iteration 312: 0.0021797482688104457\n",
      "Training cost of iteration 313: 0.002176871377808318\n",
      "Training cost of iteration 314: 0.0021740676323986586\n",
      "Training cost of iteration 315: 0.002171335172838541\n",
      "Training cost of iteration 316: 0.0021686721866694156\n",
      "Training cost of iteration 317: 0.002166076907514883\n",
      "Training cost of iteration 318: 0.0021635476139090716\n",
      "Training cost of iteration 319: 0.0021610826281547573\n",
      "Training cost of iteration 320: 0.0021586803152105305\n",
      "Training cost of iteration 321: 0.0021563390816062857\n",
      "Training cost of iteration 322: 0.0021540573743862364\n",
      "Training cost of iteration 323: 0.002151833680078855\n",
      "Training cost of iteration 324: 0.002149666523692954\n",
      "Training cost of iteration 325: 0.0021475544677393254\n",
      "Training cost of iteration 326: 0.002145496111277246\n",
      "Training cost of iteration 327: 0.0021434900889852233\n",
      "Training cost of iteration 328: 0.002141535070255357\n",
      "Training cost of iteration 329: 0.002139629758310763\n",
      "Training cost of iteration 330: 0.0021377728893453856\n",
      "Training cost of iteration 331: 0.0021359632316857234\n",
      "Training cost of iteration 332: 0.002134199584973847\n",
      "Training cost of iteration 333: 0.002132480779371191\n",
      "Training cost of iteration 334: 0.002130805674782604\n",
      "Training cost of iteration 335: 0.002129173160100093\n",
      "Training cost of iteration 336: 0.002127582152465841\n",
      "Training cost of iteration 337: 0.00212603159655394\n",
      "Training cost of iteration 338: 0.002124520463870355\n",
      "Training cost of iteration 339: 0.00212304775207076\n",
      "Training cost of iteration 340: 0.0021216124842956476\n",
      "Training cost of iteration 341: 0.002120213708522383\n",
      "Training cost of iteration 342: 0.0021188504969337145\n",
      "Training cost of iteration 343: 0.0021175219453023566\n",
      "Training cost of iteration 344: 0.0021162271723911955\n",
      "Training cost of iteration 345: 0.002114965319368773\n",
      "Training cost of iteration 346: 0.002113735549239608\n",
      "Training cost of iteration 347: 0.002112537046289016\n",
      "Training cost of iteration 348: 0.002111369015542041\n",
      "Training cost of iteration 349: 0.0021102306822361385\n",
      "Training cost of iteration 350: 0.002109121291307278\n",
      "Training cost of iteration 351: 0.002108040106889095\n",
      "Training cost of iteration 352: 0.0021069864118247895\n",
      "Training cost of iteration 353: 0.0021059595071914465\n",
      "Training cost of iteration 354: 0.0021049587118364015\n",
      "Training cost of iteration 355: 0.0021039833619254603\n",
      "Training cost of iteration 356: 0.002103032810502556\n",
      "Training cost of iteration 357: 0.002102106427060626\n",
      "Training cost of iteration 358: 0.002101203597123384\n",
      "Training cost of iteration 359: 0.0021003237218377505\n",
      "Training cost of iteration 360: 0.0020994662175766055\n",
      "Training cost of iteration 361: 0.0020986305155516896\n",
      "Training cost of iteration 362: 0.0020978160614363015\n",
      "Training cost of iteration 363: 0.0020970223149976257\n",
      "Training cost of iteration 364: 0.0020962487497383853\n",
      "Training cost of iteration 365: 0.002095494852547614\n",
      "Training cost of iteration 366: 0.0020947601233603018\n",
      "Training cost of iteration 367: 0.0020940440748256997\n",
      "Training cost of iteration 368: 0.002093346231984076\n",
      "Training cost of iteration 369: 0.0020926661319516345\n",
      "Training cost of iteration 370: 0.0020920033236135204\n",
      "Training cost of iteration 371: 0.0020913573673245595\n",
      "Training cost of iteration 372: 0.0020907278346176637\n",
      "Training cost of iteration 373: 0.002090114307919613\n",
      "Training cost of iteration 374: 0.00208951638027408\n",
      "Training cost of iteration 375: 0.002088933655071689\n",
      "Training cost of iteration 376: 0.002088365745786946\n",
      "Training cost of iteration 377: 0.0020878122757218604\n",
      "Training cost of iteration 378: 0.0020872728777560604\n",
      "Training cost of iteration 379: 0.0020867471941033006\n",
      "Training cost of iteration 380: 0.002086234876074128\n",
      "Training cost of iteration 381: 0.0020857355838445964\n",
      "Training cost of iteration 382: 0.002085248986230853\n",
      "Training cost of iteration 383: 0.0020847747604694792\n",
      "Training cost of iteration 384: 0.0020843125920033743\n",
      "Training cost of iteration 385: 0.002083862174273134\n",
      "Training cost of iteration 386: 0.0020834232085136894\n",
      "Training cost of iteration 387: 0.0020829954035561406\n",
      "Training cost of iteration 388: 0.00208257847563462\n",
      "Training cost of iteration 389: 0.0020821721481980725\n",
      "Training cost of iteration 390: 0.0020817761517268126\n",
      "Training cost of iteration 391: 0.0020813902235537565\n",
      "Training cost of iteration 392: 0.002081014107690184\n",
      "Training cost of iteration 393: 0.0020806475546559524\n",
      "Training cost of iteration 394: 0.002080290321314\n",
      "Training cost of iteration 395: 0.0020799421707090792\n",
      "Training cost of iteration 396: 0.0020796028719105865\n",
      "Training cost of iteration 397: 0.0020792721998593813\n",
      "Training cost of iteration 398: 0.0020789499352184913\n",
      "Training cost of iteration 399: 0.002078635864227652\n",
      "Training cost of iteration 400: 0.0020783297785614803\n",
      "Training cost of iteration 401: 0.0020780314751913335\n",
      "Training cost of iteration 402: 0.0020777407562506027\n",
      "Training cost of iteration 403: 0.002077457428903486\n",
      "Training cost of iteration 404: 0.0020771813052170756\n",
      "Training cost of iteration 405: 0.0020769122020366966\n",
      "Training cost of iteration 406: 0.002076649940864429\n",
      "Training cost of iteration 407: 0.002076394347740695\n",
      "Training cost of iteration 408: 0.0020761452531288783\n",
      "Training cost of iteration 409: 0.0020759024918028716\n",
      "Training cost of iteration 410: 0.0020756659027374768\n",
      "Training cost of iteration 411: 0.002075435329001602\n",
      "Training cost of iteration 412: 0.0020752106176541527\n",
      "Training cost of iteration 413: 0.002074991619642603\n",
      "Training cost of iteration 414: 0.002074778189704123\n",
      "Training cost of iteration 415: 0.002074570186269212\n",
      "Training cost of iteration 416: 0.002074367471367818\n",
      "Training cost of iteration 417: 0.002074169910537798\n",
      "Training cost of iteration 418: 0.0020739773727357436\n",
      "Training cost of iteration 419: 0.002073789730250057\n",
      "Training cost of iteration 420: 0.002073606858616229\n",
      "Training cost of iteration 421: 0.0020734286365342944\n",
      "Training cost of iteration 422: 0.0020732549457883605\n",
      "Training cost of iteration 423: 0.002073085671168206\n",
      "Training cost of iteration 424: 0.002072920700392854\n",
      "Training cost of iteration 425: 0.0020727599240360924\n",
      "Training cost of iteration 426: 0.002072603235453896\n",
      "Training cost of iteration 427: 0.002072450530713692\n",
      "Training cost of iteration 428: 0.0020723017085254106\n",
      "Training cost of iteration 429: 0.00207215667017431\n",
      "Training cost of iteration 430: 0.002072015319455488\n",
      "Training cost of iteration 431: 0.002071877562610075\n",
      "Training cost of iteration 432: 0.0020717433082630436\n",
      "Training cost of iteration 433: 0.002071612467362592\n",
      "Training cost of iteration 434: 0.002071484953121082\n",
      "Training cost of iteration 435: 0.002071360680957475\n",
      "Training cost of iteration 436: 0.0020712395684412206\n",
      "Training cost of iteration 437: 0.002071121535237583\n",
      "Training cost of iteration 438: 0.0020710065030543536\n",
      "Training cost of iteration 439: 0.0020708943955899195\n",
      "Training cost of iteration 440: 0.0020707851384826597\n",
      "Training cost of iteration 441: 0.0020706786592616135\n",
      "Training cost of iteration 442: 0.0020705748872984005\n",
      "Training cost of iteration 443: 0.0020704737537603915\n",
      "Training cost of iteration 444: 0.0020703751915650435\n",
      "Training cost of iteration 445: 0.0020702791353353926\n",
      "Training cost of iteration 446: 0.002070185521356709\n",
      "Training cost of iteration 447: 0.002070094287534218\n",
      "Training cost of iteration 448: 0.0020700053733519216\n",
      "Training cost of iteration 449: 0.002069918719832448\n",
      "Training cost of iteration 450: 0.0020698342694979487\n",
      "Training cost of iteration 451: 0.002069751966331954\n",
      "Training cost of iteration 452: 0.002069671755742226\n",
      "Training cost of iteration 453: 0.002069593584524551\n",
      "Training cost of iteration 454: 0.002069517400827442\n",
      "Training cost of iteration 455: 0.0020694431541177457\n",
      "Training cost of iteration 456: 0.0020693707951471283\n",
      "Training cost of iteration 457: 0.0020693002759194054\n",
      "Training cost of iteration 458: 0.0020692315496587056\n",
      "Training cost of iteration 459: 0.00206916457077844\n",
      "Training cost of iteration 460: 0.0020690992948510767\n",
      "Training cost of iteration 461: 0.0020690356785786606\n",
      "Training cost of iteration 462: 0.0020689736797640935\n",
      "Training cost of iteration 463: 0.0020689132572831543\n",
      "Training cost of iteration 464: 0.002068854371057207\n",
      "Training cost of iteration 465: 0.00206879698202663\n",
      "Training cost of iteration 466: 0.002068741052124893\n",
      "Training cost of iteration 467: 0.002068686544253329\n",
      "Training cost of iteration 468: 0.002068633422256502\n",
      "Training cost of iteration 469: 0.0020685816508982375\n",
      "Training cost of iteration 470: 0.002068531195838252\n",
      "Training cost of iteration 471: 0.002068482023609368\n",
      "Training cost of iteration 472: 0.0020684341015953216\n",
      "Training cost of iteration 473: 0.002068387398009123\n",
      "Training cost of iteration 474: 0.002068341881871971\n",
      "Training cost of iteration 475: 0.0020682975229927127\n",
      "Training cost of iteration 476: 0.002068254291947807\n",
      "Training cost of iteration 477: 0.0020682121600618174\n",
      "Training cost of iteration 478: 0.002068171099388383\n",
      "Training cost of iteration 479: 0.0020681310826916894\n",
      "Training cost of iteration 480: 0.00206809208342839\n",
      "Training cost of iteration 481: 0.0020680540757300227\n",
      "Training cost of iteration 482: 0.002068017034385822\n",
      "Training cost of iteration 483: 0.0020679809348260253\n",
      "Training cost of iteration 484: 0.0020679457531055474\n",
      "Training cost of iteration 485: 0.002067911465888126\n",
      "Training cost of iteration 486: 0.00206787805043082\n",
      "Training cost of iteration 487: 0.0020678454845689345\n",
      "Training cost of iteration 488: 0.002067813746701318\n",
      "Training cost of iteration 489: 0.002067782815776033\n",
      "Training cost of iteration 490: 0.0020677526712763903\n",
      "Training cost of iteration 491: 0.0020677232932073434\n",
      "Training cost of iteration 492: 0.002067694662082225\n",
      "Training cost of iteration 493: 0.002067666758909819\n",
      "Training cost of iteration 494: 0.0020676395651817673\n",
      "Training cost of iteration 495: 0.002067613062860288\n",
      "Training cost of iteration 496: 0.0020675872343662188\n",
      "Training cost of iteration 497: 0.002067562062567342\n",
      "Training cost of iteration 498: 0.0020675375307670444\n",
      "Training cost of iteration 499: 0.0020675136226932143\n",
      "Training cost of iteration 500: 0.002067490322487469\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x27d96fb6880>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, y = load_data()\n",
    "beta0, beta1 = gradient_descent(x, y)\n",
    "\n",
    "# Plot the data AND the line\n",
    "def f(x):\n",
    "    return beta0 + beta1 * x\n",
    "\n",
    "plt.plot(x, f(x))\n",
    "plt.plot(x, y, 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Multi Linear Regression on California House Pricing Dataset using 5-fold Cross Validation (50 points)\n",
    "\n",
    "In this part, you are going to train a Multi Linear Regression Model on a real dataset! The dataset we are going to use is **California House Pricing Dataset**. The target is to predict the median house value in California, given the 8 features ( You can examine the name of the features when you run the load_data function given below ). In this part, you will also use Cross-Validation with 5 folds, in addition to everything you implemented in Part 1. You will also alter your code so that it works with multiple attribute. In this case, you will have 8 features and you can code up assuming you will always have 8 features. At the end, you will report your results. \n",
    "\n",
    "In summary, you will need to do the following:\n",
    "\n",
    "- Code up gradient descent with Cross Validation for Multi Linear Regression\n",
    "- Find the best working learning rate and number of iterations setup.\n",
    "- Report the MSE loss periodically during training in folds (For example, if you are training for 1000 steps, you can print MSE loss for each 100 steps. If you are training for 100000 steps ( which you can, if you'd like ), you can print MSE loss for each 10000 steps. In total, if you print the loss 10 times periodically for each fold, it's fine ).\n",
    "- Save the final MSE results on the testing set on each fold in an array. Print the average testing MSE losses at the end of the algorithm. Additionally, print the variance of the testing MSE losses. \n",
    "\n",
    "In this part, we are giving you only the load_data function, so you are flying solo for Multi Linear Regression! It is up to you to find the working learning rate and number of iterations setup. In addition to the libraries imported for Part 1, you are allowed to use sklearn for fetching dataset (which is already done for you), and the KFold class for the cross-validation. Other than that, you are not allowed to use sklearn or any other additional libraries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import KFold\n",
    "def load_data():\n",
    "    data= fetch_california_housing()\n",
    "    print(\"Features of Boston Housing Prices dataset: \", data.get(\"feature_names\"))\n",
    "    print(\"Shape of the X: \", data.data.shape)\n",
    "    print(\"Shape of the Y: \", data.target.shape)\n",
    "    return data.data, data.target\n",
    "\n",
    "\n",
    "\n",
    "def multiple_gradient_descent_KFold(x, y, learning_rate=0.00000001, max_its=10000):\n",
    "    w = np.zeros(x.shape[1])\n",
    "    b = 0\n",
    "    mse_history = np.zeros(0)\n",
    "    kfold = KFold(n_splits=5)\n",
    "    fold_idx = 1\n",
    "    for train_idx, test_idx in kfold.split(x):\n",
    "        print(f\"### FOLD {fold_idx} ###\")\n",
    "        fold_idx += 1\n",
    "        x_train = x[train_idx]\n",
    "        y_train = y[train_idx]\n",
    "        for i in range(max_its):\n",
    "            y_diff = np.dot(x_train, w.T) + b - y_train\n",
    "            w -= learning_rate * np.dot(x_train.T, y_diff) / len(y_train)\n",
    "            b -= learning_rate * np.sum(y_diff) / len(y_train)\n",
    "            if (i + 1) % (max_its / 10) == 0:\n",
    "                print(f\"Training cost of iteration {i + 1}:\", np.dot(y_diff.T, y_diff) / (2 * len(y_train)))\n",
    "        x_test = x[test_idx]\n",
    "        y_test = y[test_idx]\n",
    "        y_test_diff = b + np.dot(x_test, w.T) - y_test\n",
    "        mse_history = np.append(mse_history, np.dot(y_test_diff.T, y_test_diff) / (2 * len(y_test)))\n",
    "    print(\"Average test loss: \", np.average(mse_history))\n",
    "    print(\"Variance of test loss: \", np.var(mse_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features of Boston Housing Prices dataset:  ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
      "Shape of the X:  (20640, 8)\n",
      "Shape of the Y:  (20640,)\n",
      "### FOLD 1 ###\n",
      "Training cost of iteration 1000: 1.5429712013608172\n",
      "Training cost of iteration 2000: 1.4351910261228098\n",
      "Training cost of iteration 3000: 1.340615189369442\n",
      "Training cost of iteration 4000: 1.2576255152397222\n",
      "Training cost of iteration 5000: 1.1848021336680974\n",
      "Training cost of iteration 6000: 1.120899178211557\n",
      "Training cost of iteration 7000: 1.0648234620832366\n",
      "Training cost of iteration 8000: 1.015615767416339\n",
      "Training cost of iteration 9000: 0.9724344275096795\n",
      "Training cost of iteration 10000: 0.9345409210522816\n",
      "### FOLD 2 ###\n",
      "Training cost of iteration 1000: 0.8415296871310223\n",
      "Training cost of iteration 2000: 0.8207056732590008\n",
      "Training cost of iteration 3000: 0.8025071125472651\n",
      "Training cost of iteration 4000: 0.7866024288488835\n",
      "Training cost of iteration 5000: 0.7727019220263328\n",
      "Training cost of iteration 6000: 0.7605524792707682\n",
      "Training cost of iteration 7000: 0.7499329543489333\n",
      "Training cost of iteration 8000: 0.740650130422569\n",
      "Training cost of iteration 9000: 0.7325351927387507\n",
      "Training cost of iteration 10000: 0.7254406467975888\n",
      "### FOLD 3 ###\n",
      "Training cost of iteration 1000: 0.6617396790411154\n",
      "Training cost of iteration 2000: 0.6579739695679999\n",
      "Training cost of iteration 3000: 0.6546470880154107\n",
      "Training cost of iteration 4000: 0.6517074227438165\n",
      "Training cost of iteration 5000: 0.6491094327022872\n",
      "Training cost of iteration 6000: 0.6468129334015894\n",
      "Training cost of iteration 7000: 0.6447824668716311\n",
      "Training cost of iteration 8000: 0.6429867457249582\n",
      "Training cost of iteration 9000: 0.6413981626098959\n",
      "Training cost of iteration 10000: 0.6399923573621616\n",
      "### FOLD 4 ###\n",
      "Training cost of iteration 1000: 0.6980783028692443\n",
      "Training cost of iteration 2000: 0.6954886140473363\n",
      "Training cost of iteration 3000: 0.6932022645144365\n",
      "Training cost of iteration 4000: 0.6911832540828119\n",
      "Training cost of iteration 5000: 0.6893998551554582\n",
      "Training cost of iteration 6000: 0.6878241056443842\n",
      "Training cost of iteration 7000: 0.6864313620706161\n",
      "Training cost of iteration 8000: 0.6851999057034076\n",
      "Training cost of iteration 9000: 0.6841105954438318\n",
      "Training cost of iteration 10000: 0.6831465619050116\n",
      "### FOLD 5 ###\n",
      "Training cost of iteration 1000: 0.642152578016629\n",
      "Training cost of iteration 2000: 0.6417947695159314\n",
      "Training cost of iteration 3000: 0.6414770184041415\n",
      "Training cost of iteration 4000: 0.641194372430129\n",
      "Training cost of iteration 5000: 0.6409424919305348\n",
      "Training cost of iteration 6000: 0.6407175740525294\n",
      "Training cost of iteration 7000: 0.640516286350237\n",
      "Training cost of iteration 8000: 0.6403357085952968\n",
      "Training cost of iteration 9000: 0.6401732817854726\n",
      "Training cost of iteration 10000: 0.6400267634609113\n",
      "Average test loss:  0.6884965656339614\n",
      "Variance of test loss:  0.010874046495309594\n"
     ]
    }
   ],
   "source": [
    "x, y = load_data()\n",
    "multiple_gradient_descent_KFold(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Probability Questions (20 Points)\n",
    "\n",
    "In this part, you are given 2 probability questions that you will solve by hand ( You do not code anything ). You can either share your answer via Markdown Cells, or you can insert an image of your hand-written solution on a paper. To insert an image, after changing the cell type to Markdown, click Edit -> Insert Image. This way, you do not have to keep the image in local, it will be directly embedded in the notebook file. Try to keep your answers as illustrative as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Istanbul is a busy city, and unfortunately busy cities tend to have busy traffics. In Istanbul, one in every two days, there is a traffic jam on the main roads, if the weather is sunny. However if it rains, it gets worse! Three in every four rainy days, main roads are jammed with traffic! \n",
    "\n",
    "Our beloved lecturer, Assoc. Prof. Yusuf Yaslan is a very responsible lecturer, however sometimes life happens, and he might arrive late to the class. If the weather is rainy and there is a traffic jam, the probability of him being late is 20%. Otherwise, he is late to only 5% of the classes.\n",
    "\n",
    "Given a randomly selected day:\n",
    "\n",
    "- What is the probability that the weather is rainy, there is no traffic jam and Prof. Yaslan is NOT late to the class?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given information:\n",
    "- P(sunny) = 0.5\n",
    "- P(rainy) = 0.5\n",
    "- P(jam | sunny) = 0.5\n",
    "- P(jam | rainy) = 0.75\n",
    "- P(late | rainy  jam) = 0.2\n",
    "- P(late | no rain or no jam) = 0.05\n",
    "\n",
    "Question:\n",
    "- What is P(rainy) * (1 - P(jam | rainy)) * (1 - P(late | no jam))?\n",
    "\n",
    "Answer is 0.5 * 0.25 * 0.95 = 0.11875"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "I am a magician, and as every magician, I have a tricky coin that results in heads 75% of the times when tossed instead of the usual 50%. However, one day, I put this tricky coin next to a regular coin and left it there, and now I can't remember which one is which! Nevertheless, I took one of the coins, not knowing if I took tricky coin or the regular coin. You can assume that the regular coin has a 50% chance of landing on heads. Now I toss the coin I took 10 times and record the results.\n",
    "\n",
    "- What is the probability of getting exactly 4 heads?\n",
    "- What is the probability of getting at least 9 heads?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In one toss, probability of getting a head is,\n",
    "\n",
    "(Probability of having the regular coin * Probability of getting a head in regular coin) + (Probability of having the tricky coin * Probability of getting a head in tricky coin), which is:\n",
    "\n",
    "0.5 * 0.5 + 0.5 * 0.75 = 0.625\n",
    "\n",
    "- Probability of getting exactly 4 heads:\n",
    "\n",
    "$(0.625)^4$ * $(0.375)^6$ * $\\binom{10}{4}$\t= 0.08911010787\n",
    "\n",
    "It is multiplied with combination because of the orderings of different outcomes that satisfy the condition.\n",
    "\n",
    "- Probability of getting at least 9 heads = Prob. of getting exactly 9 heads + Prob. of getting exactly 10 heads:\n",
    "\n",
    "($(0.625)^9$ * 0.375 * $\\binom{10}{9}$) + ($(0.625)^{10}$ * $\\binom{10}{10}$) = 0.06366462901"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
